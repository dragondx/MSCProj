{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define this if you have more than 1 gpu\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, util\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p,q):\n",
    "    return np.sum(p * (np.log2(p)-np.log2(q)))\n",
    "\n",
    "def js_divergence(p,q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "def one_hot(p,q):\n",
    "    return 1 if p==q else 0\n",
    "\n",
    "\n",
    "def sigmoid(val):\n",
    "    # return val\n",
    "    return 1/(1+np.exp(-17*(val-0.5)))\n",
    "\n",
    "def identity(val):\n",
    "    return val\n",
    "\n",
    "# js ensure symmetric\n",
    "def similarity(p,q,mode=\"js\", func=identity):\n",
    "    if mode == \"js\":\n",
    "        return func(np.exp2(-js_divergence(np.array(p),np.array(q))))\n",
    "    elif mode == \"kl\":\n",
    "        return func(np.exp2(-kl_divergence(np.array(p),np.array(q))))\n",
    "    elif mode == \"one-hot\":\n",
    "        return one_hot(p,q)\n",
    "\n",
    "def get_random_index_pairs(num_data, amount):\n",
    "    return np.random.randint(num_data, size=(amount, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten to one list for all 3\n",
    "with open('train_data.pickle', 'rb') as file:\n",
    "    train = pickle.load(file)\n",
    "\n",
    "with open('gpt.pickle', 'rb') as file:\n",
    "    gpt = pickle.load(file)\n",
    "    \n",
    "with open('gpt_p2.pickle', 'rb') as file:\n",
    "    gpt2 = pickle.load(file)\n",
    "\n",
    "gpt = [item for sublist in gpt for item in sublist]\n",
    "gpt2 = [item for sublist in gpt2 for item in sublist]\n",
    "\n",
    "mixed = gpt + gpt2\n",
    "test = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('./res/sbert_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('evaluation.pickle', 'rb') as file:\n",
    "    eval_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"banking\",\"valuation\",\"household\",\"real estate\",\"corporate\",\"external\",\"sovereign\",\"technology\", \"climate\", \"energy\", \"health\", \"eu\"]\n",
    "\n",
    "#cosine similarity\n",
    "#Compute embedding for both lists\n",
    "embedded_class_dictionary = {label: [] for label in classes}\n",
    "\n",
    "\n",
    "for label in classes:\n",
    "    for sentence in eval_dict[label]:\n",
    "        embeddings = model.encode(sentence, convert_to_tensor=True)\n",
    "        embedded_class_dictionary[label].append(embeddings)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def rescale(dist):\n",
    "    beta = torch.mean(dist[math.ceil(0.25*len(dist)):math.floor(0.75*len(dist))])\n",
    "    alpha = torch.max(torch.tensor([(10+1/(torch.std(dist)/torch.mean(dist))), 400]))\n",
    "    return 1/(1+torch.exp(-alpha*(dist-beta)))\n",
    "\n",
    "def query(text, examples=10):\n",
    "    scores = []\n",
    "    text_vector = model.encode(text, convert_to_tensor=True)\n",
    "    for label in classes:\n",
    "        if label != \"eu\":\n",
    "            examples_list = random.sample(embedded_class_dictionary[label], k=examples)\n",
    "        else:\n",
    "            examples_list = embedded_class_dictionary[label]\n",
    "        cosine_scores = torch.tensor([util.pytorch_cos_sim(text_vector,  example) for example in examples_list])\n",
    "        scores.append(torch.mean(cosine_scores))\n",
    "    # torch.nn.functional.softmax(torch.tensor(scores))\n",
    "    scores = torch.tensor(scores)\n",
    "    scores = scores/torch.sum(scores)\n",
    "    scores = rescale(scores)\n",
    "    scores = scores/torch.sum(scores)\n",
    "    #softmax\n",
    "    return {label:score for label, score in zip(classes,scores)}, np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8185]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"In contrast to the radical forces buffeting valuations, for most companies, 2020 was a year of “strategy lockdown.\"\n",
    "sent2 = \"Climate change is a real thing.\"\n",
    "\n",
    "u = model.encode(sent1)\n",
    "v = model.encode(sent2)\n",
    "util.pytorch_cos_sim(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'banking': tensor(0.0018),\n",
       "  'valuation': tensor(0.1601),\n",
       "  'household': tensor(0.0124),\n",
       "  'real estate': tensor(0.0516),\n",
       "  'corporate': tensor(0.1664),\n",
       "  'external': tensor(0.1011),\n",
       "  'sovereign': tensor(0.0059),\n",
       "  'technology': tensor(0.0579),\n",
       "  'climate': tensor(0.1490),\n",
       "  'energy': tensor(0.0793),\n",
       "  'health': tensor(0.1463),\n",
       "  'eu': tensor(0.0682)},\n",
       " array([0.0018129 , 0.16008356, 0.01243954, 0.05163125, 0.1663717 ,\n",
       "        0.10113119, 0.00590704, 0.0579321 , 0.14897409, 0.07926115,\n",
       "        0.14625244, 0.06820304], dtype=float32))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query(\"In contrast to the radical forces buffeting valuations, for most companies, 2020 was a year of “strategy lockdown.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'banking': tensor(0.0034),\n",
       "  'valuation': tensor(0.1556),\n",
       "  'household': tensor(0.1686),\n",
       "  'real estate': tensor(0.2710),\n",
       "  'corporate': tensor(0.0097),\n",
       "  'external': tensor(0.0700),\n",
       "  'sovereign': tensor(0.0062),\n",
       "  'technology': tensor(0.0042),\n",
       "  'climate': tensor(0.1854),\n",
       "  'energy': tensor(0.0073),\n",
       "  'health': tensor(0.1020),\n",
       "  'eu': tensor(0.0167)},\n",
       " array([0.00338506, 0.15561585, 0.1686111 , 0.27097803, 0.00974991,\n",
       "        0.06995411, 0.0061908 , 0.00415097, 0.18535496, 0.00727346,\n",
       "        0.10199468, 0.01674107], dtype=float32))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"Mortgage interest rate in selected European countries as of 4th quarter of 2019 and 2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,top_k_accuracy_score\n",
    "def compute_metrics(labels, preds):\n",
    "    best = np.argmax(preds, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, best, average='macro')\n",
    "    acc = accuracy_score(labels, best)\n",
    "    top3 = top_k_accuracy_score(labels, preds ,k=3)\n",
    "    top2 = top_k_accuracy_score(labels, preds ,k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'top3': top3,\n",
    "        'top2': top2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [00:07<00:00, 33.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "labels = []\n",
    "preds = []\n",
    "\n",
    "for item in tqdm(test):\n",
    "    labels.append(np.argmax(np.array(item[\"dist\"])))\n",
    "    preds.append(query(item[\"text\"])[1])\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(labels) == len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7662835249042146,\n",
       " 'f1': 0.7614132751514595,\n",
       " 'precision': 0.7893137567453573,\n",
       " 'recall': 0.7917546391230602,\n",
       " 'top3': 0.9386973180076629,\n",
       " 'top2': 0.8735632183908046}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(labels,preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>speakers</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Isabel Schnabel</td>\n",
       "      <td>Societal responsibility and central bank indep...</td>\n",
       "      <td>Keynote speech by Isabel Schnabel, Member of t...</td>\n",
       "      <td>SPEECH  Societal responsibility and central...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>Climate change and financial integration</td>\n",
       "      <td>Keynote speech by Luis de Guindos, Vice-Presid...</td>\n",
       "      <td>SPEECH  Climate change and financial integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>The ECB strategy review</td>\n",
       "      <td>Presentation by Philip R. Lane, Member of the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>Fabio Panetta</td>\n",
       "      <td>At the edge of tomorrow: preparing the future ...</td>\n",
       "      <td>Introductory remarks by Fabio Panetta, Member ...</td>\n",
       "      <td>SPEECH  At the edge of tomorrow: preparing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>Towards a green capital markets union for Europe</td>\n",
       "      <td>Speech by Christine Lagarde, President of the ...</td>\n",
       "      <td>SPEECH  Towards a green capital markets uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           speakers  \\\n",
       "0  2021-05-27    Isabel Schnabel   \n",
       "1  2021-05-27    Luis de Guindos   \n",
       "2  2021-05-25     Philip R. Lane   \n",
       "3  2021-05-19      Fabio Panetta   \n",
       "4  2021-05-06  Christine Lagarde   \n",
       "\n",
       "                                               title  \\\n",
       "0  Societal responsibility and central bank indep...   \n",
       "1           Climate change and financial integration   \n",
       "2                            The ECB strategy review   \n",
       "3  At the edge of tomorrow: preparing the future ...   \n",
       "4   Towards a green capital markets union for Europe   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  Keynote speech by Isabel Schnabel, Member of t...   \n",
       "1  Keynote speech by Luis de Guindos, Vice-Presid...   \n",
       "2  Presentation by Philip R. Lane, Member of the ...   \n",
       "3  Introductory remarks by Fabio Panetta, Member ...   \n",
       "4  Speech by Christine Lagarde, President of the ...   \n",
       "\n",
       "                                            contents  \n",
       "0     SPEECH  Societal responsibility and central...  \n",
       "1     SPEECH  Climate change and financial integr...  \n",
       "2                                                NaN  \n",
       "3     SPEECH  At the edge of tomorrow: preparing ...  \n",
       "4     SPEECH  Towards a green capital markets uni...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "speeches = pd.read_csv('./all_ECB_speeches.csv', delimiter='|')\n",
    "speeches.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Remove NA entries\n",
    "speeches = speeches.dropna()\n",
    "\n",
    "#Only get presidential speeches\n",
    "# speeches = speeches.loc[speeches.subtitle.str.contains(\"\\sPresident\\s\"),:]\n",
    "\n",
    "\n",
    "#Regex cleaning\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('SPEECH', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\s+', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\(.*?\\)', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\[.*?\\]', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Note.*?\\.', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Chart .*?\\..*?\\.', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('I\\..*?References', ' ', regex=True) #edge caSe\n",
    "speeches['contents'] = speeches['contents'].replace('References.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('REFERENCES.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('LITERATURE.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('BIBLIOGRAPHY.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace(' [0-9]\\. ', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Vol.*?pp.*?\\.', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Vol\\..*?[0-9]*,.*?No\\..*?\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\..*?\\.', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('See.*?\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('SEE ALSO.*', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your kind attention\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your attention\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('I thank you for your attention\\..*', ' ', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "# speeches['contents'] = speeches['contents'].replace('[^\\x00-\\x7F]+', ' ', regex=True)\n",
    "\n",
    "# can also clean more edge cases like Thank you./Thank you for your kind attention. etc. kill everything behind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2460\n",
      " \n",
      "2266\n"
     ]
    }
   ],
   "source": [
    "# remove non-english\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "def isEnglish(text):\n",
    "    try:\n",
    "        if detect(text) == 'en':\n",
    "            return True\n",
    "        else:\n",
    "            # print(text[:40])\n",
    "            return False\n",
    "    except:\n",
    "        print(text)\n",
    "        return False\n",
    "\n",
    "def isLongerThan(text):\n",
    "    return len(text)>500\n",
    "\n",
    "def filter(text):\n",
    "    return isEnglish(text) and isLongerThan(text)\n",
    "\n",
    "print(len(speeches))\n",
    "speeches = speeches[speeches.apply(lambda x: filter(x['contents']), axis=1)]   \n",
    "print(len(speeches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# pre-processing functions\n",
    "\n",
    "def preprocess(speech):\n",
    "    return tokenize.sent_tokenize(speech)\n",
    "\n",
    "def join_to_fit(tokens):\n",
    "    results = []\n",
    "    temp = \"\"\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "\n",
    "        if count >= 500:\n",
    "\n",
    "            results.append(temp[:500])\n",
    "            temp = temp[500:]\n",
    "            count = 0 \n",
    "\n",
    "        count += len(token)\n",
    "        temp += token\n",
    "\n",
    "    return results\n",
    "\n",
    "# tried president only (same)\n",
    "# removed neutral sentences (same)\n",
    "# fss alternative index: #neg sent - #pos sent / total\n",
    "def analyze_topic(speech):\n",
    "    dists = []\n",
    "  \n",
    "    print(f\"Number of Sentences: {len(speech)}\")\n",
    "    for index, paragraph in enumerate(speech):\n",
    "        # print(f\"Sentence processed:{(index+1)/len(speech)} Sentence Length:{len(paragraph)}\" )\n",
    "        # print(paragraph)\n",
    "        out, dist = query(paragraph)\n",
    "        dists.append(dist)\n",
    "\n",
    "    return np.arrary(dists)\n",
    "   \n",
    "count = 0\n",
    "def complete_topic(speech):\n",
    "    global count\n",
    "    count +=1\n",
    "    print(f\"Document processed: {count}\")\n",
    "    tokenized_speeches = preprocess(speech)\n",
    "    # tokenized_speeches = join_to_fit(tokenized_speeches)\n",
    "    outputs = analyze_topic(tokenized_speeches)\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_and_concat(dataframe, field, func, column_names):\n",
    "    return pd.concat((\n",
    "        dataframe,\n",
    "        dataframe[field].apply(\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n",
    "\n",
    "# speeches['mean'], speeches['std'] = speeches.apply(lambda speech: sentiment_analysis(speech.contents), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processed: 1\n",
      "Number of Sentences: 115\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arrary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/623000954.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mecb_with_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_and_concat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeeches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'contents'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomplete_topic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"dist\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/2841364323.py\u001b[0m in \u001b[0;36mapply_and_concat\u001b[1;34m(dataframe, field, func, column_names)\u001b[0m\n\u001b[0;32m      2\u001b[0m     return pd.concat((\n\u001b[0;32m      3\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         dataframe[field].apply(\n\u001b[0m\u001b[0;32m      5\u001b[0m             lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/2841364323.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(cell)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         dataframe[field].apply(\n\u001b[1;32m----> 5\u001b[1;33m             lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# speeches['mean'], speeches['std'] = speeches.apply(lambda speech: sentiment_analysis(speech.contents), axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/1226870484.py\u001b[0m in \u001b[0;36mcomplete_topic\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mtokenized_speeches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# tokenized_speeches = join_to_fit(tokenized_speeches)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_speeches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20660/1226870484.py\u001b[0m in \u001b[0;36manalyze_topic\u001b[1;34m(speech)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0m\u001b[0;32m    314\u001b[0m                                  \"{!r}\".format(__name__, attr))\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arrary'"
     ]
    }
   ],
   "source": [
    "ecb_with_topics = apply_and_concat(speeches, 'contents', complete_topic, [\"dist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb_with_topics.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb_with_topics.to_csv('./ecb_with_topics_sbert.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b174d106187cf4c36b481e4f9775d9b6103c84ebc5bd0573d3cc5e86d067305"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

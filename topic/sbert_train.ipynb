{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, util\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import numpy as np\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def kl_divergence(p,q):\r\n",
    "    return np.sum(p * (np.log2(p)-np.log2(q)))\r\n",
    "\r\n",
    "def js_divergence(p,q):\r\n",
    "    m = 0.5 * (p + q)\r\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\r\n",
    "\r\n",
    "def one_hot(p,q):\r\n",
    "    return 1 if p==q else 0\r\n",
    "\r\n",
    "def sigmoid(val):\r\n",
    "    return 1/(1+np.exp(-17*(val-0.5)))\r\n",
    "\r\n",
    "# js ensure symmetric\r\n",
    "def similarity(p,q,mode=\"js\"):\r\n",
    "    if mode == \"js\":\r\n",
    "        return sigmoid(np.exp2(-js_divergence(np.array(p),np.array(q))))\r\n",
    "    elif mode == \"kl\":\r\n",
    "        return sigmoid(np.exp2(-kl_divergence(np.array(p),np.array(q))))\r\n",
    "    elif mode == \"one-hot\":\r\n",
    "        return one_hot(p,q)\r\n",
    "\r\n",
    "def get_random_index_pairs(num_data, amount):\r\n",
    "    return np.random.randint(num_data, size=(amount, 2))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# flatten to one list for all 3\r\n",
    "with open('train_data.pickle', 'rb') as file:\r\n",
    "    train = pickle.load(file)\r\n",
    "\r\n",
    "with open('gpt.pickle', 'rb') as file:\r\n",
    "    gpt = pickle.load(file)\r\n",
    "    \r\n",
    "with open('gpt.pickle', 'rb') as file:\r\n",
    "    gpt2 = pickle.load(file)\r\n",
    "\r\n",
    "gpt = [item for sublist in gpt for item in sublist]\r\n",
    "gpt2 = [item for sublist in gpt2 for item in sublist]\r\n",
    "\r\n",
    "mixed = gpt[200:] + train + gpt2[200:]\r\n",
    "test = gpt2[:200] + gpt[:200]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from itertools import combinations\r\n",
    "import random\r\n",
    "all_pairs = list(combinations(range(len(mixed)),2))\r\n",
    "\r\n",
    "random.shuffle(all_pairs)\r\n",
    "# bert load data\r\n",
    "data = [{\"texts\":[mixed[idx[0]][\"text\"],mixed[idx[1]][\"text\"]], \"label\": similarity(mixed[idx[0]][\"dist\"],mixed[idx[1]][\"dist\"])} for idx in all_pairs]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "len(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3646350"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train, dev = data[:450000],data[450000:500000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pair1 = [item[\"texts\"][0] for item in dev]\r\n",
    "pair2 = [item[\"texts\"][1] for item in dev]\r\n",
    "scores = [float(item[\"label\"]) for item in dev]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Define your train examples. You need more than just two examples...\r\n",
    "train_examples = [InputExample(texts=item[\"texts\"], label=float(item[\"label\"])) for item in train]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\r\n",
    "\r\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\r\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\r\n",
    "# model = SentenceTransformer('./')\r\n",
    "\r\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(pair1, pair2, scores)\r\n",
    "\r\n",
    "\r\n",
    "#Define your train dataset, the dataloader and the train loss\r\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\r\n",
    "train_loss = losses.CosineSimilarityLoss(model)\r\n",
    "\r\n",
    "#Tune the model\r\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=4, warmup_steps=100, evaluator=evaluator,evaluation_steps=500)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Iteration:   6%|â–Œ         | 794/14063 [03:37<1:00:28,  3.66it/s]\n",
      "Epoch:   0%|          | 0/4 [03:37<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22260/2177869706.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#Tune the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_objectives\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mevaluation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[0;32m    702\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                         \u001b[0mloss_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save(\"./sbert\",\"sbert_fin\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "with open('evaluation.pickle', 'rb') as file:\r\n",
    "    eval_dict = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classes = [\"banking\",\"valuation\",\"household\",\"real estate\",\"corporate\",\"external\",\"sovereign\",\"technology\", \"climate\", \"energy\", \"health\", \"eu\"]\r\n",
    "#cosine similarity\r\n",
    "#Compute embedding for both lists\r\n",
    "embedded_class_dictionary = {label: [] for label in classes}\r\n",
    "\r\n",
    "\r\n",
    "for label in classes:\r\n",
    "    for sentence in eval_dict[label]:\r\n",
    "        embeddings = model.encode(sentence, convert_to_tensor=True)\r\n",
    "        embedded_class_dictionary[label].append(embeddings)\r\n",
    "\r\n",
    "  \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# with open('embedded_class_dictionary.pickle', 'wb') as file:\r\n",
    "#     pickle.dump(embedded_class_dictionary, file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "import torch\r\n",
    "\r\n",
    "def query(text, examples=10):\r\n",
    "    scores = []\r\n",
    "    text_vector = model.encode(text, convert_to_tensor=True)\r\n",
    "    for label in classes:\r\n",
    "        if label != \"eu\":\r\n",
    "            examples_list = random.sample(embedded_class_dictionary[label], k=examples)\r\n",
    "        else:\r\n",
    "            examples_list = embedded_class_dictionary[label]\r\n",
    "        cosine_scores = torch.tensor([util.pytorch_cos_sim(text_vector,  example) for example in examples_list])\r\n",
    "        scores.append(torch.mean(cosine_scores))\r\n",
    "    # torch.nn.functional.softmax(torch.tensor(scores))\r\n",
    "    scores = torch.tensor(scores)\r\n",
    "    scores = scores/torch.sum(scores)\r\n",
    "    #softmax\r\n",
    "    return {label:score for label, score in zip(classes,scores)}\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "query(\"In contrast to the radical forces buffeting valuations, for most companies, 2020 was a year of â€œstrategy lockdown.\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,top_k_accuracy_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_metrics(labels, preds):\r\n",
    "    best = \r\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\r\n",
    "    acc = accuracy_score(labels, preds)\r\n",
    "    top3 = top_k_accuracy_score(labels, pred.predictions,k=3)\r\n",
    "    top2 = top_k_accuracy_score(labels, pred.predictions,k=2)\r\n",
    "    return {\r\n",
    "        'accuracy': acc,\r\n",
    "        'f1': f1,\r\n",
    "        'precision': precision,\r\n",
    "        'recall': recall,\r\n",
    "        'top3': top3,\r\n",
    "        'top2': top2\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for dictionary in test:\r\n",
    "    \r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (Temp/ipykernel_4840/2262350480.py, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\B\\AppData\\Local\\Temp/ipykernel_4840/2262350480.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for dictionary in tes:\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "6b31b91393804ba8dc9c9bfc0276b237c748fbfc9d237f295d2f8dc3a0ae1c04"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
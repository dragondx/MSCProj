{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "\r\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation, util\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import numpy as np\r\n",
    "import pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def kl_divergence(p,q):\r\n",
    "    return np.sum(p * (np.log2(p)-np.log2(q)))\r\n",
    "\r\n",
    "def js_divergence(p,q):\r\n",
    "    m = 0.5 * (p + q)\r\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\r\n",
    "\r\n",
    "def one_hot(p,q):\r\n",
    "    return 1 if p==q else 0\r\n",
    "\r\n",
    "def sigmoid(val):\r\n",
    "    return val\r\n",
    "    return 1/(1+np.exp(-17*(val-0.5)))\r\n",
    "\r\n",
    "# js ensure symmetric\r\n",
    "def similarity(p,q,mode=\"js\"):\r\n",
    "    if mode == \"js\":\r\n",
    "        return sigmoid(np.exp2(-js_divergence(np.array(p),np.array(q))))\r\n",
    "    elif mode == \"kl\":\r\n",
    "        return sigmoid(np.exp2(-kl_divergence(np.array(p),np.array(q))))\r\n",
    "    elif mode == \"one-hot\":\r\n",
    "        return one_hot(p,q)\r\n",
    "\r\n",
    "def get_random_index_pairs(num_data, amount):\r\n",
    "    return np.random.randint(num_data, size=(amount, 2))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# flatten to one list for all 3\r\n",
    "with open('train_data.pickle', 'rb') as file:\r\n",
    "    train = pickle.load(file)\r\n",
    "\r\n",
    "with open('gpt.pickle', 'rb') as file:\r\n",
    "    gpt = pickle.load(file)\r\n",
    "    \r\n",
    "with open('gpt_p2.pickle', 'rb') as file:\r\n",
    "    gpt2 = pickle.load(file)\r\n",
    "\r\n",
    "gpt = [item for sublist in gpt for item in sublist]\r\n",
    "gpt2 = [item for sublist in gpt2 for item in sublist]\r\n",
    "\r\n",
    "mixed = gpt[200:] + train + gpt2[200:]\r\n",
    "test = gpt2[:400] + gpt[:200]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model = SentenceTransformer('./res/sbert_v2')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "with open('evaluation.pickle', 'rb') as file:\r\n",
    "    eval_dict = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "classes = [\"banking\",\"valuation\",\"household\",\"real estate\",\"corporate\",\"external\",\"sovereign\",\"technology\", \"climate\", \"energy\", \"health\", \"eu\"]\r\n",
    "#cosine similarity\r\n",
    "#Compute embedding for both lists\r\n",
    "embedded_class_dictionary = {label: [] for label in classes}\r\n",
    "\r\n",
    "\r\n",
    "for label in classes:\r\n",
    "    for sentence in eval_dict[label]:\r\n",
    "        embeddings = model.encode(sentence, convert_to_tensor=True)\r\n",
    "        embedded_class_dictionary[label].append(embeddings)\r\n",
    "\r\n",
    "  \r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\abc\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
      "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import random\r\n",
    "import torch\r\n",
    "import math\r\n",
    "\r\n",
    "def rescale(dist):\r\n",
    "    beta = torch.mean(dist[math.ceil(0.25*len(dist)):math.floor(0.75*len(dist))])\r\n",
    "    alpha = torch.max(torch.tensor([(10+1/(torch.std(dist)/torch.mean(dist))), 400]))\r\n",
    "    return 1/(1+torch.exp(-alpha*(dist-beta)))\r\n",
    "\r\n",
    "def query(text, examples=10):\r\n",
    "    scores = []\r\n",
    "    text_vector = model.encode(text, convert_to_tensor=True)\r\n",
    "    for label in classes:\r\n",
    "        if label != \"eu\":\r\n",
    "            examples_list = random.sample(embedded_class_dictionary[label], k=examples)\r\n",
    "        else:\r\n",
    "            examples_list = embedded_class_dictionary[label]\r\n",
    "        cosine_scores = torch.tensor([util.pytorch_cos_sim(text_vector,  example) for example in examples_list])\r\n",
    "        scores.append(torch.mean(cosine_scores))\r\n",
    "    # torch.nn.functional.softmax(torch.tensor(scores))\r\n",
    "    scores = torch.tensor(scores)\r\n",
    "    scores = scores/torch.sum(scores)\r\n",
    "    scores = rescale(scores)\r\n",
    "    scores = scores/torch.sum(scores)\r\n",
    "    #softmax\r\n",
    "    return {label:score for label, score in zip(classes,scores)}, np.array(scores)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sent1 = \"In contrast to the radical forces buffeting valuations, for most companies, 2020 was a year of “strategy lockdown.\"\r\n",
    "sent2 = \"Climate change is a real thing.\"\r\n",
    "\r\n",
    "u = model.encode(sent1)\r\n",
    "v = model.encode(sent2)\r\n",
    "util.pytorch_cos_sim(u,v)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.5756]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\r\n",
    "query(\"In contrast to the radical forces buffeting valuations, for most companies, 2020 was a year of “strategy lockdown.\")\r\n",
    "# {'banking': tensor(0.0144),\r\n",
    "#   'valuation': tensor(0.3418),\r\n",
    "#   'household': tensor(0.0022),\r\n",
    "#   'real estate': tensor(0.0181),\r\n",
    "#   'corporate': tensor(0.5484),\r\n",
    "#   'external': tensor(0.0063),\r\n",
    "#   'sovereign': tensor(0.0193),\r\n",
    "#   'technology': tensor(0.0143),\r\n",
    "#   'climate': tensor(0.0077),\r\n",
    "#   'energy': tensor(0.0056),\r\n",
    "#   'health': tensor(0.0210),\r\n",
    "#   'eu': tensor(0.0008)}\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'banking': tensor(0.0219),\n",
       "  'valuation': tensor(0.3215),\n",
       "  'household': tensor(0.0336),\n",
       "  'real estate': tensor(0.0320),\n",
       "  'corporate': tensor(0.3360),\n",
       "  'external': tensor(0.0153),\n",
       "  'sovereign': tensor(0.0923),\n",
       "  'technology': tensor(0.0119),\n",
       "  'climate': tensor(0.0258),\n",
       "  'energy': tensor(0.0363),\n",
       "  'health': tensor(0.0422),\n",
       "  'eu': tensor(0.0313)},\n",
       " array([0.02186355, 0.3215338 , 0.03357272, 0.03196084, 0.33601692,\n",
       "        0.01530513, 0.09226985, 0.01192343, 0.02581508, 0.03628198,\n",
       "        0.04220223, 0.03125454], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "query(\"Mortgage interest rate in selected European countries as of 4th quarter of 2019 and 2020\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'banking': tensor(0.0166),\n",
       "  'valuation': tensor(0.0362),\n",
       "  'household': tensor(0.2336),\n",
       "  'real estate': tensor(0.5644),\n",
       "  'corporate': tensor(0.0096),\n",
       "  'external': tensor(0.0324),\n",
       "  'sovereign': tensor(0.0198),\n",
       "  'technology': tensor(0.0088),\n",
       "  'climate': tensor(0.0067),\n",
       "  'energy': tensor(0.0258),\n",
       "  'health': tensor(0.0427),\n",
       "  'eu': tensor(0.0034)},\n",
       " array([0.01663781, 0.03619295, 0.23357873, 0.56437606, 0.00961803,\n",
       "        0.03238049, 0.01980924, 0.00879645, 0.00666669, 0.02579491,\n",
       "        0.04270546, 0.00344312], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\r\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,top_k_accuracy_score\r\n",
    "def compute_metrics(labels, preds):\r\n",
    "    best = np.argmax(preds, axis=1)\r\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, best, average='macro')\r\n",
    "    acc = accuracy_score(labels, best)\r\n",
    "    top3 = top_k_accuracy_score(labels, preds ,k=3)\r\n",
    "    top2 = top_k_accuracy_score(labels, preds ,k=2)\r\n",
    "    return {\r\n",
    "        'accuracy': acc,\r\n",
    "        'f1': f1,\r\n",
    "        'precision': precision,\r\n",
    "        'recall': recall,\r\n",
    "        'top3': top3,\r\n",
    "        'top2': top2\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from tqdm import tqdm\r\n",
    "labels = []\r\n",
    "preds = []\r\n",
    "\r\n",
    "for item in tqdm(gpt2):\r\n",
    "    labels.append(np.argmax(np.array(item[\"dist\"])))\r\n",
    "    preds.append(query(item[\"text\"])[1])\r\n",
    "preds = np.array(preds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1038/1038 [00:41<00:00, 25.10it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "assert len(labels) == len(preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "compute_metrics(labels,preds)\r\n",
    "# {'accuracy': 0.92,\r\n",
    "#  'f1': 0.8316229467944879,\r\n",
    "#  'precision': 0.8142551892551894,\r\n",
    "#  'recall': 0.8727039627039627,\r\n",
    "#  'top3': 0.9725,\r\n",
    "#  'top2': 0.965}\r\n",
    "# train\r\n",
    "# {'accuracy': 0.5770712909441233,\r\n",
    "#  'f1': 0.421304669340332,\r\n",
    "#  'precision': 0.41728348913927044,\r\n",
    "#  'recall': 0.44001393966077357,\r\n",
    "#  'top3': 0.8420038535645472,\r\n",
    "#  'top2': 0.7620423892100193}\r\n",
    "# test\r\n",
    "# {'accuracy': 0.930281690140845,\r\n",
    "#  'f1': 0.8722745634176153,\r\n",
    "#  'precision': 0.8698194847154102,\r\n",
    "#  'recall': 0.8845044288758909,\r\n",
    "#  'top3': 0.9852112676056338,\r\n",
    "#  'top2': 0.9781690140845071}\r\n",
    "\r\n",
    "\r\n",
    "# enhance train accu\r\n",
    "# {'accuracy': 0.9478873239436619,\r\n",
    "#  'f1': 0.9068146034650141,\r\n",
    "#  'precision': 0.893095646375258,\r\n",
    "#  'recall': 0.9287405747114715,\r\n",
    "#  'top3': 0.9852112676056338,\r\n",
    "#  'top2': 0.9795774647887324}\r\n",
    "\r\n",
    "# test\r\n",
    "# {'accuracy': 0.7083333333333334,\r\n",
    "#  'f1': 0.6400212205154878,\r\n",
    "#  'precision': 0.6549408332820279,\r\n",
    "#  'recall': 0.6365285737194072,\r\n",
    "#  'top3': 0.915,\r\n",
    "#  'top2': 0.865}"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'accuracy': 0.7745664739884393,\n",
       " 'f1': 0.6091591343856654,\n",
       " 'precision': 0.6673696421068095,\n",
       " 'recall': 0.6195302496240477,\n",
       " 'top3': 0.941233140655106,\n",
       " 'top2': 0.8978805394990366}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import pandas as pd\r\n",
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import tqdm\r\n",
    "\r\n",
    "\r\n",
    "import random\r\n",
    "\r\n",
    "from pprint import pprint\r\n",
    "import pickle \r\n",
    "\r\n",
    "\r\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\r\n",
    "\r\n",
    "import ssl\r\n",
    "\r\n",
    "try:\r\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\r\n",
    "except AttributeError:\r\n",
    "    pass\r\n",
    "else:\r\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\r\n",
    "\r\n",
    "\r\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n",
    "import torch \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\r\n",
    "speeches = pd.read_csv('./all_ECB_speeches.csv', delimiter='|')\r\n",
    "speeches.head()\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>speakers</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Isabel Schnabel</td>\n",
       "      <td>Societal responsibility and central bank indep...</td>\n",
       "      <td>Keynote speech by Isabel Schnabel, Member of t...</td>\n",
       "      <td>SPEECH  Societal responsibility and central...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>Climate change and financial integration</td>\n",
       "      <td>Keynote speech by Luis de Guindos, Vice-Presid...</td>\n",
       "      <td>SPEECH  Climate change and financial integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>The ECB strategy review</td>\n",
       "      <td>Presentation by Philip R. Lane, Member of the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>Fabio Panetta</td>\n",
       "      <td>At the edge of tomorrow: preparing the future ...</td>\n",
       "      <td>Introductory remarks by Fabio Panetta, Member ...</td>\n",
       "      <td>SPEECH  At the edge of tomorrow: preparing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>Towards a green capital markets union for Europe</td>\n",
       "      <td>Speech by Christine Lagarde, President of the ...</td>\n",
       "      <td>SPEECH  Towards a green capital markets uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           speakers  \\\n",
       "0  2021-05-27    Isabel Schnabel   \n",
       "1  2021-05-27    Luis de Guindos   \n",
       "2  2021-05-25     Philip R. Lane   \n",
       "3  2021-05-19      Fabio Panetta   \n",
       "4  2021-05-06  Christine Lagarde   \n",
       "\n",
       "                                               title  \\\n",
       "0  Societal responsibility and central bank indep...   \n",
       "1           Climate change and financial integration   \n",
       "2                            The ECB strategy review   \n",
       "3  At the edge of tomorrow: preparing the future ...   \n",
       "4   Towards a green capital markets union for Europe   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  Keynote speech by Isabel Schnabel, Member of t...   \n",
       "1  Keynote speech by Luis de Guindos, Vice-Presid...   \n",
       "2  Presentation by Philip R. Lane, Member of the ...   \n",
       "3  Introductory remarks by Fabio Panetta, Member ...   \n",
       "4  Speech by Christine Lagarde, President of the ...   \n",
       "\n",
       "                                            contents  \n",
       "0     SPEECH  Societal responsibility and central...  \n",
       "1     SPEECH  Climate change and financial integr...  \n",
       "2                                                NaN  \n",
       "3     SPEECH  At the edge of tomorrow: preparing ...  \n",
       "4     SPEECH  Towards a green capital markets uni...  "
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\r\n",
    "#Remove NA entries\r\n",
    "speeches = speeches.dropna()\r\n",
    "\r\n",
    "#Only get presidential speeches\r\n",
    "# speeches = speeches.loc[speeches.subtitle.str.contains(\"\\sPresident\\s\"),:]\r\n",
    "\r\n",
    "\r\n",
    "#Regex cleaning\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('SPEECH', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('\\s+', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('\\(.*?\\)', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('\\[.*?\\]', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('Note.*?\\.', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('Chart .*?\\..*?\\.', ' ', regex=True)\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('I\\..*?References', ' ', regex=True) #edge caSe\r\n",
    "speeches['contents'] = speeches['contents'].replace('References.*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('REFERENCES.*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('LITERATURE.*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('BIBLIOGRAPHY.*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace(' [0-9]\\. ', ' ', regex=True)\r\n",
    "\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('Vol.*?pp.*?\\.', ' ', regex=True)\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('Vol\\..*?[0-9]*,.*?No\\..*?\\.', ' ', regex=True)\r\n",
    "\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\..*?\\.', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\.', ' ', regex=True)\r\n",
    "\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('See.*?\\.', ' ', regex=True)\r\n",
    "\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('SEE ALSO.*', ' ', regex=True)\r\n",
    "\r\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you\\..*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your kind attention\\..*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your attention\\..*', ' ', regex=True)\r\n",
    "speeches['contents'] = speeches['contents'].replace('I thank you for your attention\\..*', ' ', regex=True)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# speeches['contents'] = speeches['contents'].replace('[^\\x00-\\x7F]+', ' ', regex=True)\r\n",
    "\r\n",
    "# can also clean more edge cases like Thank you./Thank you for your kind attention. etc. kill everything behind "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# remove non-english\r\n",
    "\r\n",
    "from langdetect import detect\r\n",
    "\r\n",
    "def isEnglish(text):\r\n",
    "    try:\r\n",
    "        if detect(text) == 'en':\r\n",
    "            return True\r\n",
    "        else:\r\n",
    "            # print(text[:40])\r\n",
    "            return False\r\n",
    "    except:\r\n",
    "        print(text)\r\n",
    "        return False\r\n",
    "\r\n",
    "def isLongerThan(text):\r\n",
    "    return len(text)>500\r\n",
    "\r\n",
    "def filter(text):\r\n",
    "    return isEnglish(text) and isLongerThan(text)\r\n",
    "\r\n",
    "# non_en_idx = []\r\n",
    "# for i in range(len(speeches)):\r\n",
    "#     if not isEnglish(speeches.iloc[i]['contents']):\r\n",
    "#         non_en_idx.append(i)\r\n",
    "\r\n",
    "        \r\n",
    "\r\n",
    "# print(len(non_en_idx))\r\n",
    "print(len(speeches))\r\n",
    "speeches = speeches[speeches.apply(lambda x: filter(x['contents']), axis=1)]   \r\n",
    "print(len(speeches))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langdetect'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5092/633670761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# remove non-english\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0misEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langdetect'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\r\n",
    "from nltk import tokenize\r\n",
    "\r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "# pre-processing functions\r\n",
    "\r\n",
    "def preprocess(speech):\r\n",
    "    return tokenize.sent_tokenize(speech)\r\n",
    "\r\n",
    "def join_to_fit(tokens):\r\n",
    "    results = []\r\n",
    "    temp = \"\"\r\n",
    "    count = 0\r\n",
    "    for token in tokens:\r\n",
    "\r\n",
    "        if count >= 500:\r\n",
    "\r\n",
    "            results.append(temp[:500])\r\n",
    "            temp = temp[500:]\r\n",
    "            count = 0 \r\n",
    "\r\n",
    "        count += len(token)\r\n",
    "        temp += token\r\n",
    "\r\n",
    "    return results\r\n",
    "\r\n",
    "# tried president only (same)\r\n",
    "# removed neutral sentences (same)\r\n",
    "# fss alternative index: #neg sent - #pos sent / total\r\n",
    "def analyze_topic(speech):\r\n",
    "    dists = []\r\n",
    "  \r\n",
    "    print(f\"Number of Sentences: {len(speech)}\")\r\n",
    "    for index, paragraph in enumerate(speech):\r\n",
    "        # print(f\"Sentence processed:{(index+1)/len(speech)} Sentence Length:{len(paragraph)}\" )\r\n",
    "        # print(paragraph)\r\n",
    "        out, dist = query(paragraph)\r\n",
    "        dists.append(dist)\r\n",
    "\r\n",
    "    return np.arrary(dists)\r\n",
    "   \r\n",
    "count = 0\r\n",
    "def complete_topic(speech):\r\n",
    "    global count\r\n",
    "    count +=1\r\n",
    "    print(f\"Document processed: {count}\")\r\n",
    "    tokenized_speeches = preprocess(speech)\r\n",
    "    # tokenized_speeches = join_to_fit(tokenized_speeches)\r\n",
    "    outputs = analyze_topic(tokenized_speeches)\r\n",
    "    return outputs\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def apply_and_concat(dataframe, field, func, column_names):\r\n",
    "    return pd.concat((\r\n",
    "        dataframe,\r\n",
    "        dataframe[field].apply(\r\n",
    "            lambda cell: pd.Series(func(cell), index=column_names))), axis=1)\r\n",
    "\r\n",
    "# speeches['mean'], speeches['std'] = speeches.apply(lambda speech: sentiment_analysis(speech.contents), axis=1)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecb_with_topics = apply_and_concat(speeches, 'contents', complete_topic, [\"dist\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecb_with_topics.iloc[0]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ecb_with_topics.to_csv('./ecb_with_topics_sbert.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "8e81d6174cb081d483213be829e7831163670ee6c9fd24781192b6d714da88ab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
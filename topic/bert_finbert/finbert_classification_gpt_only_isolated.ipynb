{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the training script for fine-tuning bert on \n",
    "# unaltered GPT data and manually labelled data\n",
    "# For better performance/generalization, look for augmented dataset\n",
    "# Read README.md for comments and details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all classes\n",
    "\n",
    "classes = [\"banking\",\"valuation\",\"household\",\"real estate\",\"corporate\",\"external\",\"sovereign\",\"technology\", \"climate\", \"energy\", \"health\", \"eu\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,top_k_accuracy_score\n",
    "import math\n",
    "import pickle\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load bert-based and finbert\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(classes))\n",
    "finbert = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert', use_fast =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights transfer for encoder layers only \n",
    "finbert_weights = finbert.state_dict()\n",
    "model_weights = model.state_dict()\n",
    "del finbert_weights[\"bert.pooler.dense.weight\"]\n",
    "del finbert_weights[\"bert.pooler.dense.bias\"]\n",
    "del finbert_weights[\"classifier.weight\"]\n",
    "del finbert_weights[\"classifier.bias\"]\n",
    "finbert_weights[\"bert.pooler.dense.weight\"] = model_weights[\"bert.pooler.dense.weight\"]\n",
    "finbert_weights[\"bert.pooler.dense.bias\"] = model_weights[\"bert.pooler.dense.bias\"]\n",
    "finbert_weights[\"classifier.weight\"] = model_weights[\"classifier.weight\"]\n",
    "finbert_weights[\"classifier.bias\"] = model_weights[\"classifier.bias\"]\n",
    "\n",
    "model.load_state_dict(finbert_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten to one list for all 3\n",
    "\n",
    "# manual labelled\n",
    "with open('train_data.pickle', 'rb') as file:\n",
    "    train = pickle.load(file)\n",
    "\n",
    "# gpt labelled p1\n",
    "with open('gpt.pickle', 'rb') as file:\n",
    "    gpt = pickle.load(file)\n",
    "\n",
    "# gpt labelled p2\n",
    "with open('gpt_p2.pickle', 'rb') as file:\n",
    "    gpt2 = pickle.load(file)\n",
    "    \n",
    "gpt = [item for sublist in gpt for item in sublist]\n",
    "gpt2 = [item for sublist in gpt2 for item in sublist]\n",
    "\n",
    "mixed = gpt + gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2458\n"
     ]
    }
   ],
   "source": [
    "print(len(mixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to sampling methods instead of argmax if treat GPT-3 data as probabilities.\n",
    "sample = 1\n",
    "\n",
    "text_max = [item[\"text\"] for i in range(sample) for item in train]\n",
    "label_max = [np.argmax(item[\"dist\"]) for i in range(sample) for item in train]\n",
    "\n",
    "text_max_mixed = [item[\"text\"] for i in range(sample) for item in mixed]\n",
    "label_max_mixed = [np.argmax(item[\"dist\"]) for i in range(sample) for item in mixed]\n",
    "\n",
    "comb_text = text_max_mixed\n",
    "comb_label = label_max_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(comb_text) == len(comb_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "import random\n",
    "temp = list(zip(comb_text, comb_label))\n",
    "random.shuffle(temp)\n",
    "comb_text, comb_label = zip(*temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2458\n"
     ]
    }
   ],
   "source": [
    "print(len(comb_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = comb_text[:math.ceil(len(comb_text)*0.9)], comb_text[math.ceil(len(comb_text)*0.9):]\n",
    "train_label, test_label = comb_label[:math.ceil(len(comb_label)*0.9)], comb_label[math.ceil(len(comb_label)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\"text\":train_text, \"label\":train_label})\n",
    "test_dataset = Dataset.from_dict({\"text\":test_text, \"label\":test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.37ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 90.89ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1037,  2488,  ...,     0,     0,     0],\n",
       "        [  101,  9944,  2181,  ...,     0,     0,     0],\n",
       "        [  101,  2045,  2323,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  4826,  1010,  ...,     0,     0,     0],\n",
       "        [  101,  2714, 10069,  ...,     0,     0,     0],\n",
       "        [  101,  4613,  2011,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    top3 = top_k_accuracy_score(labels, pred.predictions,k=3)\n",
    "    top2 = top_k_accuracy_score(labels, pred.predictions,k=2)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'top3': top3,\n",
    "         'top2': top2\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy = \"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit = 1, # Only last 5 models are saved. Older ones are deleted.\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 2213\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2780\n",
      "  5%|▍         | 138/2780 [00:22<06:56,  6.34it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  5%|▌         | 139/2780 [00:23<06:56,  6.34it/s]Saving model checkpoint to ./results\\checkpoint-139\n",
      "Configuration saved in ./results\\checkpoint-139\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2275032997131348, 'eval_accuracy': 0.2653061224489796, 'eval_f1': 0.05849293465392536, 'eval_precision': 0.06293809700372548, 'eval_recall': 0.08994708994708994, 'eval_top3': 0.5959183673469388, 'eval_top2': 0.46938775510204084, 'eval_runtime': 0.3134, 'eval_samples_per_second': 781.818, 'eval_steps_per_second': 12.764, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-139\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-2618] due to args.save_total_limit\n",
      " 10%|▉         | 277/2780 [00:46<06:13,  6.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 10%|█         | 278/2780 [00:46<06:13,  6.70it/s]Saving model checkpoint to ./results\\checkpoint-278\n",
      "Configuration saved in ./results\\checkpoint-278\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8715399503707886, 'eval_accuracy': 0.4163265306122449, 'eval_f1': 0.12697716513505986, 'eval_precision': 0.10321704578594341, 'eval_recall': 0.16820987654320987, 'eval_top3': 0.6408163265306123, 'eval_top2': 0.5306122448979592, 'eval_runtime': 0.2999, 'eval_samples_per_second': 816.823, 'eval_steps_per_second': 13.336, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-278\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-3080] due to args.save_total_limit\n",
      " 15%|█▍        | 416/2780 [01:08<05:49,  6.76it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 15%|█▌        | 417/2780 [01:08<05:49,  6.76it/s]Saving model checkpoint to ./results\\checkpoint-417\n",
      "Configuration saved in ./results\\checkpoint-417\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5200684070587158, 'eval_accuracy': 0.5469387755102041, 'eval_f1': 0.2503530311402597, 'eval_precision': 0.26100223686930696, 'eval_recall': 0.2699123855037833, 'eval_top3': 0.7918367346938775, 'eval_top2': 0.7020408163265306, 'eval_runtime': 0.3013, 'eval_samples_per_second': 813.081, 'eval_steps_per_second': 13.275, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-417\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-139] due to args.save_total_limit\n",
      " 18%|█▊        | 501/2780 [01:22<05:14,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8874, 'learning_rate': 2e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 555/2780 [01:30<05:30,  6.73it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 20%|██        | 556/2780 [01:31<05:30,  6.73it/s]Saving model checkpoint to ./results\\checkpoint-556\n",
      "Configuration saved in ./results\\checkpoint-556\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3819433450698853, 'eval_accuracy': 0.5551020408163265, 'eval_f1': 0.3188806483463684, 'eval_precision': 0.39156318827371456, 'eval_recall': 0.3177545585139671, 'eval_top3': 0.8204081632653061, 'eval_top2': 0.726530612244898, 'eval_runtime': 0.3181, 'eval_samples_per_second': 770.109, 'eval_steps_per_second': 12.573, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-556\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-278] due to args.save_total_limit\n",
      " 25%|██▍       | 694/2780 [01:53<05:05,  6.83it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 25%|██▌       | 695/2780 [01:53<05:05,  6.83it/s]Saving model checkpoint to ./results\\checkpoint-695\n",
      "Configuration saved in ./results\\checkpoint-695\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4581735134124756, 'eval_accuracy': 0.5755102040816327, 'eval_f1': 0.3599842954966915, 'eval_precision': 0.42091934298323297, 'eval_recall': 0.36072011172817625, 'eval_top3': 0.7755102040816326, 'eval_top2': 0.7061224489795919, 'eval_runtime': 0.2986, 'eval_samples_per_second': 820.372, 'eval_steps_per_second': 13.394, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-695\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-417] due to args.save_total_limit\n",
      " 30%|██▉       | 833/2780 [02:16<04:28,  7.25it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 30%|███       | 834/2780 [02:16<04:28,  7.25it/s]Saving model checkpoint to ./results\\checkpoint-834\n",
      "Configuration saved in ./results\\checkpoint-834\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.540336012840271, 'eval_accuracy': 0.5591836734693878, 'eval_f1': 0.3557663612617173, 'eval_precision': 0.4424158570897701, 'eval_recall': 0.3416300694526501, 'eval_top3': 0.8244897959183674, 'eval_top2': 0.7142857142857143, 'eval_runtime': 0.2942, 'eval_samples_per_second': 832.893, 'eval_steps_per_second': 13.598, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-834\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-556] due to args.save_total_limit\n",
      " 35%|███▍      | 972/2780 [02:38<04:29,  6.70it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 35%|███▌      | 973/2780 [02:38<04:29,  6.70it/s]Saving model checkpoint to ./results\\checkpoint-973\n",
      "Configuration saved in ./results\\checkpoint-973\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7193952798843384, 'eval_accuracy': 0.5469387755102041, 'eval_f1': 0.3401250429089562, 'eval_precision': 0.35141594516594515, 'eval_recall': 0.3512200165425972, 'eval_top3': 0.7673469387755102, 'eval_top2': 0.6653061224489796, 'eval_runtime': 0.2993, 'eval_samples_per_second': 818.683, 'eval_steps_per_second': 13.366, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-973\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-834] due to args.save_total_limit\n",
      " 36%|███▌      | 1001/2780 [02:44<04:14,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6868, 'learning_rate': 1.56140350877193e-05, 'epoch': 7.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1111/2780 [03:01<04:14,  6.57it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 40%|████      | 1112/2780 [03:01<04:13,  6.57it/s]Saving model checkpoint to ./results\\checkpoint-1112\n",
      "Configuration saved in ./results\\checkpoint-1112\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8059898614883423, 'eval_accuracy': 0.5714285714285714, 'eval_f1': 0.3483436226971616, 'eval_precision': 0.37123497658616267, 'eval_recall': 0.35008749961706953, 'eval_top3': 0.7918367346938775, 'eval_top2': 0.7061224489795919, 'eval_runtime': 0.307, 'eval_samples_per_second': 797.935, 'eval_steps_per_second': 13.028, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1112\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-973] due to args.save_total_limit\n",
      " 45%|████▍     | 1250/2780 [03:23<03:44,  6.81it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 45%|████▌     | 1251/2780 [03:23<03:44,  6.81it/s]Saving model checkpoint to ./results\\checkpoint-1251\n",
      "Configuration saved in ./results\\checkpoint-1251\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9232038259506226, 'eval_accuracy': 0.5551020408163265, 'eval_f1': 0.35279981873331284, 'eval_precision': 0.36122939560439565, 'eval_recall': 0.3605815045667196, 'eval_top3': 0.7877551020408163, 'eval_top2': 0.7183673469387755, 'eval_runtime': 0.284, 'eval_samples_per_second': 862.632, 'eval_steps_per_second': 14.084, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1251\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1112] due to args.save_total_limit\n",
      " 50%|████▉     | 1389/2780 [03:45<03:15,  7.11it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 50%|█████     | 1390/2780 [03:46<03:15,  7.11it/s]Saving model checkpoint to ./results\\checkpoint-1390\n",
      "Configuration saved in ./results\\checkpoint-1390\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0355265140533447, 'eval_accuracy': 0.5755102040816327, 'eval_f1': 0.37504393779734574, 'eval_precision': 0.39033024634444513, 'eval_recall': 0.3730854159281578, 'eval_top3': 0.7673469387755102, 'eval_top2': 0.7142857142857143, 'eval_runtime': 0.2885, 'eval_samples_per_second': 849.237, 'eval_steps_per_second': 13.865, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1390\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1251] due to args.save_total_limit\n",
      " 54%|█████▍    | 1501/2780 [04:04<03:01,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1463, 'learning_rate': 1.1228070175438597e-05, 'epoch': 10.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1528/2780 [04:08<02:59,  6.97it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      " 55%|█████▌    | 1529/2780 [04:08<02:59,  6.97it/s]Saving model checkpoint to ./results\\checkpoint-1529\n",
      "Configuration saved in ./results\\checkpoint-1529\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1347901821136475, 'eval_accuracy': 0.5795918367346938, 'eval_f1': 0.370981538194653, 'eval_precision': 0.3877801120448179, 'eval_recall': 0.3759495308554448, 'eval_top3': 0.7836734693877551, 'eval_top2': 0.726530612244898, 'eval_runtime': 0.2961, 'eval_samples_per_second': 827.487, 'eval_steps_per_second': 13.51, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1529\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-695] due to args.save_total_limit\n",
      " 60%|█████▉    | 1667/2780 [04:30<02:38,  7.03it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 60%|██████    | 1668/2780 [04:30<02:38,  7.03it/s]Saving model checkpoint to ./results\\checkpoint-1668\n",
      "Configuration saved in ./results\\checkpoint-1668\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2617690563201904, 'eval_accuracy': 0.5755102040816327, 'eval_f1': 0.38435621327470854, 'eval_precision': 0.39310233156135227, 'eval_recall': 0.3863850021444107, 'eval_top3': 0.7836734693877551, 'eval_top2': 0.6979591836734694, 'eval_runtime': 0.2887, 'eval_samples_per_second': 848.637, 'eval_steps_per_second': 13.855, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1668\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1390] due to args.save_total_limit\n",
      " 65%|██████▍   | 1806/2780 [04:52<02:21,  6.89it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 65%|██████▌   | 1807/2780 [04:52<02:21,  6.89it/s]Saving model checkpoint to ./results\\checkpoint-1807\n",
      "Configuration saved in ./results\\checkpoint-1807\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.381368637084961, 'eval_accuracy': 0.5551020408163265, 'eval_f1': 0.3353155116212118, 'eval_precision': 0.36285986079641885, 'eval_recall': 0.3279865295994328, 'eval_top3': 0.7795918367346939, 'eval_top2': 0.7061224489795919, 'eval_runtime': 0.3012, 'eval_samples_per_second': 813.444, 'eval_steps_per_second': 13.281, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1807\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1668] due to args.save_total_limit\n",
      " 70%|██████▉   | 1945/2780 [05:14<01:56,  7.16it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 70%|███████   | 1946/2780 [05:14<01:56,  7.16it/s]Saving model checkpoint to ./results\\checkpoint-1946\n",
      "Configuration saved in ./results\\checkpoint-1946\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.413862705230713, 'eval_accuracy': 0.5673469387755102, 'eval_f1': 0.37501217997045977, 'eval_precision': 0.3875987554112554, 'eval_recall': 0.3741174819803852, 'eval_top3': 0.7836734693877551, 'eval_top2': 0.689795918367347, 'eval_runtime': 0.2887, 'eval_samples_per_second': 848.571, 'eval_steps_per_second': 13.854, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-1946\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1807] due to args.save_total_limit\n",
      " 72%|███████▏  | 2001/2780 [05:24<01:52,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0235, 'learning_rate': 6.842105263157896e-06, 'epoch': 14.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 2084/2780 [05:36<01:36,  7.18it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 75%|███████▌  | 2085/2780 [05:36<01:36,  7.18it/s]Saving model checkpoint to ./results\\checkpoint-2085\n",
      "Configuration saved in ./results\\checkpoint-2085\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.438964366912842, 'eval_accuracy': 0.563265306122449, 'eval_f1': 0.3579034804331658, 'eval_precision': 0.3580599575935397, 'eval_recall': 0.36332930435753014, 'eval_top3': 0.7836734693877551, 'eval_top2': 0.7020408163265306, 'eval_runtime': 0.288, 'eval_samples_per_second': 850.794, 'eval_steps_per_second': 13.891, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2085\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-1946] due to args.save_total_limit\n",
      " 80%|███████▉  | 2223/2780 [05:57<01:18,  7.14it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 80%|████████  | 2224/2780 [05:58<01:17,  7.14it/s]Saving model checkpoint to ./results\\checkpoint-2224\n",
      "Configuration saved in ./results\\checkpoint-2224\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.487133741378784, 'eval_accuracy': 0.5510204081632653, 'eval_f1': 0.37234127989135685, 'eval_precision': 0.37817673504517585, 'eval_recall': 0.37349610887917345, 'eval_top3': 0.7755102040816326, 'eval_top2': 0.7020408163265306, 'eval_runtime': 0.2972, 'eval_samples_per_second': 824.412, 'eval_steps_per_second': 13.46, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2224\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-2085] due to args.save_total_limit\n",
      " 85%|████████▍ | 2362/2780 [06:19<00:58,  7.19it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 85%|████████▌ | 2363/2780 [06:20<00:58,  7.19it/s]Saving model checkpoint to ./results\\checkpoint-2363\n",
      "Configuration saved in ./results\\checkpoint-2363\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5146498680114746, 'eval_accuracy': 0.5591836734693878, 'eval_f1': 0.35328236578236577, 'eval_precision': 0.36413035532672255, 'eval_recall': 0.3514929228974928, 'eval_top3': 0.7755102040816326, 'eval_top2': 0.7020408163265306, 'eval_runtime': 0.2905, 'eval_samples_per_second': 843.269, 'eval_steps_per_second': 13.768, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2363\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-2224] due to args.save_total_limit\n",
      " 90%|████████▉ | 2501/2780 [06:41<00:38,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0087, 'learning_rate': 2.456140350877193e-06, 'epoch': 17.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 90%|█████████ | 2502/2780 [06:41<00:38,  7.23it/s]Saving model checkpoint to ./results\\checkpoint-2502\n",
      "Configuration saved in ./results\\checkpoint-2502\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5423431396484375, 'eval_accuracy': 0.563265306122449, 'eval_f1': 0.3782422554200994, 'eval_precision': 0.38402190504270295, 'eval_recall': 0.3792707006752705, 'eval_top3': 0.7836734693877551, 'eval_top2': 0.6979591836734694, 'eval_runtime': 0.2926, 'eval_samples_per_second': 837.273, 'eval_steps_per_second': 13.67, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2502\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-2363] due to args.save_total_limit\n",
      " 95%|█████████▍| 2640/2780 [07:03<00:19,  7.01it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      " 95%|█████████▌| 2641/2780 [07:03<00:19,  7.01it/s]Saving model checkpoint to ./results\\checkpoint-2641\n",
      "Configuration saved in ./results\\checkpoint-2641\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5475220680236816, 'eval_accuracy': 0.5551020408163265, 'eval_f1': 0.347444781422565, 'eval_precision': 0.3520355550246854, 'eval_recall': 0.34880475085448204, 'eval_top3': 0.7755102040816326, 'eval_top2': 0.6979591836734694, 'eval_runtime': 0.3034, 'eval_samples_per_second': 807.405, 'eval_steps_per_second': 13.182, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2641\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-2502] due to args.save_total_limit\n",
      "100%|█████████▉| 2779/2780 [07:25<00:00,  7.01it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      "\n",
      "100%|██████████| 2780/2780 [07:26<00:00,  7.01it/s]Saving model checkpoint to ./results\\checkpoint-2780\n",
      "Configuration saved in ./results\\checkpoint-2780\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.555901288986206, 'eval_accuracy': 0.5551020408163265, 'eval_f1': 0.3685013382171906, 'eval_precision': 0.37002416972382646, 'eval_recall': 0.37137419529892646, 'eval_top3': 0.7795918367346939, 'eval_top2': 0.6979591836734694, 'eval_runtime': 0.3009, 'eval_samples_per_second': 814.113, 'eval_steps_per_second': 13.292, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results\\checkpoint-2780\\pytorch_model.bin\n",
      "Deleting older checkpoint [results\\checkpoint-2641] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-1529 (score: 0.5795918367346938).\n",
      "100%|██████████| 2780/2780 [07:28<00:00,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 448.3402, 'train_samples_per_second': 98.72, 'train_steps_per_second': 6.201, 'train_loss': 0.49576307299325795, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2780, training_loss=0.49576307299325795, metrics={'train_runtime': 448.3402, 'train_samples_per_second': 98.72, 'train_steps_per_second': 6.201, 'train_loss': 0.49576307299325795, 'epoch': 20.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 245\n",
      "  Batch size = 64\n",
      " 75%|███████▌  | 3/4 [00:00<00:00, 19.79it/s]C:\\Users\\felix\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|██████████| 4/4 [00:00<00:00, 17.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.1347901821136475,\n",
       " 'eval_accuracy': 0.5795918367346938,\n",
       " 'eval_f1': 0.370981538194653,\n",
       " 'eval_precision': 0.3877801120448179,\n",
       " 'eval_recall': 0.3759495308554448,\n",
       " 'eval_top3': 0.7836734693877551,\n",
       " 'eval_top2': 0.726530612244898,\n",
       " 'eval_runtime': 0.2973,\n",
       " 'eval_samples_per_second': 824.003,\n",
       " 'eval_steps_per_second': 13.453,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b174d106187cf4c36b481e4f9775d9b6103c84ebc5bd0573d3cc5e86d067305"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

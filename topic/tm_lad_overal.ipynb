{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.stem as stemmer\n",
    "\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n",
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1123)>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "speeches = pd.read_csv('./all_ECB_speeches.csv', delimiter='|', error_bad_lines=False)\n",
    "speeches.tail()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            date              speakers  \\\n",
       "2483  1997-05-13  Alexandre Lamfalussy   \n",
       "2484  1997-04-30  Alexandre Lamfalussy   \n",
       "2485  1997-04-22  Alexandre Lamfalussy   \n",
       "2486  1997-03-10  Alexandre Lamfalussy   \n",
       "2487  1997-02-07  Alexandre Lamfalussy   \n",
       "\n",
       "                                                  title  \\\n",
       "2483  The European Central Bank: independent and acc...   \n",
       "2484  The operation of monetary policy in stage thre...   \n",
       "2485  Convergence and the role of the European Centr...   \n",
       "2486                       Securing the benefits of EMU   \n",
       "2487  Conference organised by the Hungarian Banking ...   \n",
       "\n",
       "                                               subtitle  \\\n",
       "2483  Keynote speech delivered by Alexandre Lamfalus...   \n",
       "2484  Address by Alexandre Lamfalussy, President of ...   \n",
       "2485  Remarks by Alexandre Lamfalussy, President of ...   \n",
       "2486  Address by Alexandre Lamfalussy, President of ...   \n",
       "2487  Address by Alexandre Lamfalussy, President of ...   \n",
       "\n",
       "                                               contents  \n",
       "2483    The European Central Bank: independent and a...  \n",
       "2484    The operation of monetary policy in stage th...  \n",
       "2485    Convergence and the role of the European Cen...  \n",
       "2486    Securing the benefits of EMU   Address by Al...  \n",
       "2487    Conference organised by the Hungarian Bankin...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>speakers</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>1997-05-13</td>\n",
       "      <td>Alexandre Lamfalussy</td>\n",
       "      <td>The European Central Bank: independent and acc...</td>\n",
       "      <td>Keynote speech delivered by Alexandre Lamfalus...</td>\n",
       "      <td>The European Central Bank: independent and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>1997-04-30</td>\n",
       "      <td>Alexandre Lamfalussy</td>\n",
       "      <td>The operation of monetary policy in stage thre...</td>\n",
       "      <td>Address by Alexandre Lamfalussy, President of ...</td>\n",
       "      <td>The operation of monetary policy in stage th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>1997-04-22</td>\n",
       "      <td>Alexandre Lamfalussy</td>\n",
       "      <td>Convergence and the role of the European Centr...</td>\n",
       "      <td>Remarks by Alexandre Lamfalussy, President of ...</td>\n",
       "      <td>Convergence and the role of the European Cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>1997-03-10</td>\n",
       "      <td>Alexandre Lamfalussy</td>\n",
       "      <td>Securing the benefits of EMU</td>\n",
       "      <td>Address by Alexandre Lamfalussy, President of ...</td>\n",
       "      <td>Securing the benefits of EMU   Address by Al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>1997-02-07</td>\n",
       "      <td>Alexandre Lamfalussy</td>\n",
       "      <td>Conference organised by the Hungarian Banking ...</td>\n",
       "      <td>Address by Alexandre Lamfalussy, President of ...</td>\n",
       "      <td>Conference organised by the Hungarian Bankin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "#Remove NA entries\n",
    "speeches = speeches.dropna()\n",
    "\n",
    "print(len(speeches))\n",
    "#Only get presidential speeches\n",
    "# speeches = speeches.loc[speeches.subtitle.str.contains(\"\\sPresident\\s\"),:]\n",
    "\n",
    "\n",
    "#Regex cleaning\n",
    "speeches['contents'] = speeches['contents'].replace('SPEECH', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\s+', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\(.*?\\)', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\[.*?\\]', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Note.*?\\.', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Chart .*?\\..*?\\.', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('I\\..*?References', ' ', regex=True) #edge caSe\n",
    "speeches['contents'] = speeches['contents'].replace('References.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('REFERENCES.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('LITERATURE.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('BIBLIOGRAPHY.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace(' [0-9]\\. ', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Vol.*?pp.*?\\.', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Vol\\..*?[0-9]*,.*?No\\..*?\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\..*?\\.', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('See.*?\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('SEE ALSO.*', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your kind attention\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your attention\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('I thank you for your attention\\..*', ' ', regex=True)\n",
    "\n",
    "# reove numbers?"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2460\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "# preprocessing functions\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(nltk.tokenize.word_tokenize(str(sentence)))\n",
    "\n",
    "def remove_non_english(texts):\n",
    "    return [[w for w in nltk.wordpunct_tokenize(\" \".join(doc)) if w.lower() in words or not w.isalpha()] for doc in texts]\n",
    "\n",
    "#financial ones\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_word_length(texts):\n",
    "    return [[w for w in doc if len(w)>3] for doc in texts]\n",
    "\n",
    "def lemmatize(texts):\n",
    "    return [[ lemmatizer.lemmatize(w,pos='v') for w in doc] for doc in texts]\n",
    "\n",
    "def stemming(texts):\n",
    "    return [[nltk.ste.lemmatize(w,pos='v') for w in doc] for doc in texts]\n",
    "\n",
    "def noun_only(texts):\n",
    "    return [[word[0] for word in nltk.pos_tag(doc) if word[1] in ['NN','JJ','JJR','JJS','NNP','NNS']] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(input_data):\n",
    "    data = input_data.contents.values.tolist()\n",
    "    data.reverse()\n",
    "\n",
    "    # data = [input_data.iloc[1].contents]\n",
    "\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    data_words = remove_non_english(data_words)\n",
    "    data_words = remove_stopwords(data_words)\n",
    "    data_words = remove_word_length(data_words)\n",
    "    \n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    data_words = make_bigrams(data_words)\n",
    "\n",
    "    \n",
    "    data_words = remove_stopwords(data_words)\n",
    "    \n",
    "\n",
    "    data_words = lemmatize(data_words)\n",
    "\n",
    "    data_words = noun_only(data_words)\n",
    "\n",
    "    \n",
    "\n",
    "    return data_words\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data_words = preprocess(speeches)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "texts_bow = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "tfidf = models.TfidfModel(texts_bow)\n",
    "corpus_tfidf = tfidf[texts_bow]\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                            num_topics=11, \n",
    "                                            id2word=dictionary,\n",
    "                                            workers=4)\n",
    "coherence_model_lda = gensim.models.coherencemodel.CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "overall_res.append(coherence_model_lda.get_coherence())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "def visualize(lda_model, corpus, id2word, mode, k=5):\n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_data_filepath = os.path.join('ldavis_'+ mode +'_'+str(k))\n",
    "\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "    # load the pre-prepared pyLDAvis data from disk\n",
    "    with open(LDAvis_data_filepath, 'rb') as f:\n",
    "        LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared,'ldavis_'+ mode +'_'+ str(k) +'.html')\n",
    "    LDAvis_prepared"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "visualize(lda_model, tfidf, dictionary, \"tfidf\", 11)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j9/rqqkqhds231b5h_6lrr4xt6h0000gn/T/ipykernel_22710/1092728897.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/j9/rqqkqhds231b5h_6lrr4xt6h0000gn/T/ipykernel_22710/1197111662.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(lda_model, corpus, id2word, mode, k)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mLDAvis_data_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ldavis_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mLDAvis_prepared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDAvis_data_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLDAvis_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyLDAvis/gensim_models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyLDAvis/gensim_models.py\u001b[0m in \u001b[0;36m_extract_data\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcorpus_csc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus2csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcorpus_csc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mcorpus2csc\u001b[0;34m(corpus, num_terms, dtype, num_docs, num_nnz, printprogress)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nnz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# HACK assume feature ids fit in 32bit integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nnz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprintprogress\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdocno\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprintprogress\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PROGRESS: at document #%i/%i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mtermid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0mtermid_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mtf_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(lda_model[tfidf[9]])\n",
    "print(len(tfidf))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(13, 0.93119437)]\n",
      "735\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}
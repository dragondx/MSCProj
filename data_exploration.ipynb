{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora as corpora\n",
    "import random\n",
    "\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.stem as stemmer\n",
    "\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS as stop_words\n",
    "\n",
    "\n",
    "speeches = pd.read_csv('./all_ECB_speeches.csv', delimiter='|', error_bad_lines=False)\n",
    "speeches.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         date           speakers  \\\n",
       "0  2021-05-27    Isabel Schnabel   \n",
       "1  2021-05-27    Luis de Guindos   \n",
       "2  2021-05-25     Philip R. Lane   \n",
       "3  2021-05-19      Fabio Panetta   \n",
       "4  2021-05-06  Christine Lagarde   \n",
       "\n",
       "                                               title  \\\n",
       "0  Societal responsibility and central bank indep...   \n",
       "1           Climate change and financial integration   \n",
       "2                            The ECB strategy review   \n",
       "3  At the edge of tomorrow: preparing the future ...   \n",
       "4   Towards a green capital markets union for Europe   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  Keynote speech by Isabel Schnabel, Member of t...   \n",
       "1  Keynote speech by Luis de Guindos, Vice-Presid...   \n",
       "2  Presentation by Philip R. Lane, Member of the ...   \n",
       "3  Introductory remarks by Fabio Panetta, Member ...   \n",
       "4  Speech by Christine Lagarde, President of the ...   \n",
       "\n",
       "                                            contents  \n",
       "0     SPEECH  Societal responsibility and central...  \n",
       "1     SPEECH  Climate change and financial integr...  \n",
       "2                                                NaN  \n",
       "3     SPEECH  At the edge of tomorrow: preparing ...  \n",
       "4     SPEECH  Towards a green capital markets uni...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>speakers</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Isabel Schnabel</td>\n",
       "      <td>Societal responsibility and central bank indep...</td>\n",
       "      <td>Keynote speech by Isabel Schnabel, Member of t...</td>\n",
       "      <td>SPEECH  Societal responsibility and central...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>Climate change and financial integration</td>\n",
       "      <td>Keynote speech by Luis de Guindos, Vice-Presid...</td>\n",
       "      <td>SPEECH  Climate change and financial integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-25</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>The ECB strategy review</td>\n",
       "      <td>Presentation by Philip R. Lane, Member of the ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>Fabio Panetta</td>\n",
       "      <td>At the edge of tomorrow: preparing the future ...</td>\n",
       "      <td>Introductory remarks by Fabio Panetta, Member ...</td>\n",
       "      <td>SPEECH  At the edge of tomorrow: preparing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>Towards a green capital markets union for Europe</td>\n",
       "      <td>Speech by Christine Lagarde, President of the ...</td>\n",
       "      <td>SPEECH  Towards a green capital markets uni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "speeches.iloc[-1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "date                                               1997-02-07\n",
       "speakers                                 Alexandre Lamfalussy\n",
       "title       Conference organised by the Hungarian Banking ...\n",
       "subtitle    Address by Alexandre Lamfalussy, President of ...\n",
       "contents      Conference organised by the Hungarian Bankin...\n",
       "Name: 2487, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "speeches.iloc[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "date                                               2021-05-27\n",
       "speakers                                      Isabel Schnabel\n",
       "title       Societal responsibility and central bank indep...\n",
       "subtitle    Keynote speech by Isabel Schnabel, Member of t...\n",
       "contents       SPEECH  Societal responsibility and central...\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#28 na rows\n",
    "print(len(speeches))\n",
    "speeches = speeches.dropna()\n",
    "print(len(speeches))\n",
    "speeches.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2488\n",
      "2460\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         date           speakers  \\\n",
       "0  2021-05-27    Isabel Schnabel   \n",
       "1  2021-05-27    Luis de Guindos   \n",
       "3  2021-05-19      Fabio Panetta   \n",
       "4  2021-05-06  Christine Lagarde   \n",
       "6  2021-04-29     Frank Elderson   \n",
       "\n",
       "                                               title  \\\n",
       "0  Societal responsibility and central bank indep...   \n",
       "1           Climate change and financial integration   \n",
       "3  At the edge of tomorrow: preparing the future ...   \n",
       "4   Towards a green capital markets union for Europe   \n",
       "6  All the way to zero: guiding banks towards a c...   \n",
       "\n",
       "                                            subtitle  \\\n",
       "0  Keynote speech by Isabel Schnabel, Member of t...   \n",
       "1  Keynote speech by Luis de Guindos, Vice-Presid...   \n",
       "3  Introductory remarks by Fabio Panetta, Member ...   \n",
       "4  Speech by Christine Lagarde, President of the ...   \n",
       "6  Keynote speech by Frank Elderson, Vice-Chair o...   \n",
       "\n",
       "                                            contents  \n",
       "0     SPEECH  Societal responsibility and central...  \n",
       "1     SPEECH  Climate change and financial integr...  \n",
       "3     SPEECH  At the edge of tomorrow: preparing ...  \n",
       "4     SPEECH  Towards a green capital markets uni...  \n",
       "6     SPEECH  All the way to zero: guiding banks ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>speakers</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Isabel Schnabel</td>\n",
       "      <td>Societal responsibility and central bank indep...</td>\n",
       "      <td>Keynote speech by Isabel Schnabel, Member of t...</td>\n",
       "      <td>SPEECH  Societal responsibility and central...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-27</td>\n",
       "      <td>Luis de Guindos</td>\n",
       "      <td>Climate change and financial integration</td>\n",
       "      <td>Keynote speech by Luis de Guindos, Vice-Presid...</td>\n",
       "      <td>SPEECH  Climate change and financial integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-19</td>\n",
       "      <td>Fabio Panetta</td>\n",
       "      <td>At the edge of tomorrow: preparing the future ...</td>\n",
       "      <td>Introductory remarks by Fabio Panetta, Member ...</td>\n",
       "      <td>SPEECH  At the edge of tomorrow: preparing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>Towards a green capital markets union for Europe</td>\n",
       "      <td>Speech by Christine Lagarde, President of the ...</td>\n",
       "      <td>SPEECH  Towards a green capital markets uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-29</td>\n",
       "      <td>Frank Elderson</td>\n",
       "      <td>All the way to zero: guiding banks towards a c...</td>\n",
       "      <td>Keynote speech by Frank Elderson, Vice-Chair o...</td>\n",
       "      <td>SPEECH  All the way to zero: guiding banks ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "from langdetect import detect\n",
    "\n",
    "def isEnglish(text):\n",
    "    try:\n",
    "        if detect(text) == 'en':\n",
    "            return True\n",
    "        else:\n",
    "            # print(text[:40])\n",
    "            return False\n",
    "    except:\n",
    "        print(text)\n",
    "        return False\n",
    "\n",
    "def isLongerThan(text):\n",
    "    return len(text)>500\n",
    "\n",
    "def filter(text):\n",
    "    return isEnglish(text) and isLongerThan(text)\n",
    "\n",
    "# non_en_idx = []\n",
    "# for i in range(len(speeches)):\n",
    "#     if not isEnglish(speeches.iloc[i]['contents']):\n",
    "#         non_en_idx.append(i)\n",
    "\n",
    "        \n",
    "\n",
    "# print(len(non_en_idx))\n",
    "print(len(speeches))\n",
    "speeches = speeches[speeches.apply(lambda x: filter(x['contents']), axis=1)]   \n",
    "print(len(speeches))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2460\n",
      " \n",
      "2269\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "len(list(set(speeches.speakers.values.tolist()))) #speakers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# get index range of speeches\n",
    "\n",
    "quarters = [\"(1|2|3)\",\"(4|5|6)\", \"(7|8|9)\",\"(10|11|12)\"]\n",
    "indices = []\n",
    "for year in range(2000,2022):\n",
    "    for index, quarter in enumerate(quarters):\n",
    "        data = speeches.loc[speeches['date'].str.contains(str(year) + '-0?' + quarter + '-'), :]\n",
    "        indices.append(len(data))\n",
    "indices = indices[:-2]\n",
    "print(indices)\n",
    "print(sum(indices)/len(indices))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[14, 18, 17, 21, 14, 16, 13, 22, 20, 18, 8, 19, 12, 18, 10, 24, 16, 31, 14, 29, 13, 29, 13, 26, 20, 30, 16, 29, 21, 33, 25, 37, 29, 40, 28, 34, 26, 34, 20, 32, 22, 42, 23, 30, 26, 44, 12, 33, 10, 33, 16, 30, 27, 41, 25, 37, 20, 31, 23, 28, 18, 25, 17, 39, 23, 30, 20, 34, 34, 43, 29, 36, 28, 28, 20, 36, 30, 26, 17, 34, 20, 15, 20, 27, 19, 10]\n",
      "24.651162790697676\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/var/folders/j9/rqqkqhds231b5h_6lrr4xt6h0000gn/T/ipykernel_8747/1692635772.py:7: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  data = speeches.loc[speeches['date'].str.contains(str(year) + '-0?' + quarter + '-'), :]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "# preprocessing functions\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(nltk.tokenize.word_tokenize(str(sentence)))\n",
    "\n",
    "def remove_non_english(texts):\n",
    "    return [[w for w in nltk.wordpunct_tokenize(\" \".join(doc)) if w.lower() in words or not w.isalpha()] for doc in texts]\n",
    "\n",
    "#financial ones\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "def remove_word_length(texts):\n",
    "    return [[w for w in doc if len(w)>3] for doc in texts]\n",
    "\n",
    "def lemmatize(texts):\n",
    "    return [[ lemmatizer.lemmatize(w,pos='v') for w in doc] for doc in texts]\n",
    "\n",
    "def stemming(texts):\n",
    "    return [[nltk.ste.lemmatize(w,pos='v') for w in doc] for doc in texts]\n",
    "\n",
    "def noun_only(texts):\n",
    "    return [[word[0] for word in nltk.pos_tag(doc) if word[1] in ['NN','JJ','JJR','JJS','NNP','NNS']] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(input_data):\n",
    "    data = input_data.contents.values.tolist()\n",
    "    data.reverse()\n",
    "\n",
    "    # data = [input_data.iloc[1].contents]\n",
    "\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    data_words = remove_non_english(data_words)\n",
    "    data_words = remove_stopwords(data_words)\n",
    "    data_words = remove_word_length(data_words)\n",
    "    \n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "    data_words = make_bigrams(data_words)\n",
    "\n",
    "    \n",
    "    data_words = remove_stopwords(data_words)\n",
    "    \n",
    "\n",
    "    data_words = lemmatize(data_words)\n",
    "\n",
    "    data_words = noun_only(data_words)\n",
    "\n",
    "    \n",
    "\n",
    "    return data_words\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data = speeches.contents.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3501.9554869986778\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "#Regex cleaning\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('SPEECH', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\(.*?\\)', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('\\[.*?\\]', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Note.*?\\.', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Chart .*?\\..*?\\.', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('I\\..*?References', ' ', regex=True) #edge caSe\n",
    "speeches['contents'] = speeches['contents'].replace('References.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('REFERENCES.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('LITERATURE.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('BIBLIOGRAPHY.*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace(' [0-9]\\. ', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Vol.*?pp.*?\\.', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Vol\\..*?[0-9]*,.*?No\\..*?\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\..*?\\.', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('op\\..*?cit\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('See.*?\\.', ' ', regex=True)\n",
    "\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('SEE ALSO.*', ' ', regex=True)\n",
    "\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your kind attention\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('Thank you for your attention\\..*', ' ', regex=True)\n",
    "speeches['contents'] = speeches['contents'].replace('I thank you for your attention\\..*', ' ', regex=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "data = speeches.contents.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3122.4583516967828\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "\n",
    "data_words = remove_non_english(data_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2768.321286910533\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "data_words = remove_stopwords(data_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1029.445570736007\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "data_words = remove_word_length(data_words)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "986.177170559718\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "data_words = make_bigrams(data_words)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "963.5927721463199\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "\n",
    "data_words = lemmatize(data_words)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "953.6416923754958\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\n",
    "data_words = noun_only(data_words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "data_len = [len(i) for i in data_words]\n",
    "average_len = sum(data_len)/len(data_len)\n",
    "print(average_len)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "815.5945350374615\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
# NLP Project

2 main parts of the projects are:
1. Topic modeling
2. Sentiment analysis
3. (i know i said 2) combining the results

>This readme shall serve as a guide to using the code in this repo.


Some pointers
1. Apologies on the bad formatting
2. GPT-3 code omitted
3. Note that dataset paths might not be fixed. Please configure that before running codes.


# Dataset

Dataset used in this repo is included in their respective folders.  Codes might not reflect the correct relative paths. (Configure that before usage)

1. Raw Manually labelled topic dataset - FinTopicBank
2. Processed GPT-3 augmented dataset - x2 (gpt.pickle, gpt2.pickle )
3. Incomplete Financial Phrasebank (sentiment analysis) - complete version on huggingface

# Topic Modeling
In this repo, the guides are emphasized on the bert-based approaches. LDA codes are included nonetheless.
<br />
Note that we mixed the manually labelled data and gpt-labelled data for training.  This might not be desirable if you want to isolate/train only on gpt-labelled data and test on manually labelled data.  Reconfigurations is needed in the script in this case.

## GPT-3 Baseline

68.75% test set accuracy on the manually labelled data. We beat that after training on the data generated by GPT-3.


## Direct classification with BERT
Set the path to appropriate location (for accessing the dataset).

### Training
Run the script named training to train for either the direct classification method or the sentence bert (contrastive) method.  You can run test code to evaluate the models. (inference code included for contrastive method, it should be quite simple to extend inference code for direct classification method; details are on https://huggingface.co/transformers/main_classes/trainer.html)

For more details on the contrastive learning method (on better training objectives or alternatives), look on https://www.sbert.net/

### Results

Dataset type | Accuracies
------------ | -------------
Raw dataset (eval mixed) | 55.7% (give or take, don't use this)
Raw dataset (eval on manual data) | 57.4%
Raw dataset (eval on gpt only) | 57.9%
Augmented dataset (eval on mixed) | 81.2%
Augmented dataset (eval on manual data) | 81.9%

> manual data means the original human labelled dataset

## SBERT

n = original dataset size

new dataset size = n*(n-1)/2 

We are dealing with sentence pairs.  Similarity score defined by the exponential of the negative Jensen-Shannon divergence. More pairs used the better.

>Models are trained on 2 epochs.

Dataset type | Accuracies
------------ | -------------
Raw dataset (eval mixed) | 80% (1.5M pairs) vs 77.5% (400k pairs)
Raw dataset (eval on manual data) | 78.5% (1.5M pairs)

Notes:
1. Add augmented data to get higher accuracies.
2. Add more sentences pairs in training data for higher accuracies. (we are currently using about 50% of all pairs)
3. Don't train with sigmoid perturbed distribution. (worst performance)

<br />

# Sentiment analysis

This section is mostly just using existing works.

## VADER

This is rule-based, so no training. We just supply a dictionary to it. (though effects are negligible). 62% accuracy on the Financial Phrasebank (test set).

## FinBERT

This is already trained for us.  86% accuracy on the Financial Phrasebank. (reported on the original paper, we don't know their train-test split)


## GPT-3

86% accuracy on the Financial Phrasebank (test set). This results is zero-shot learning. (i.e. it is as good as state of the art without fine-tuning)